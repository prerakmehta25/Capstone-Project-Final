{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 422-57 Assignment 3- Prerak, Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction / Summary\n",
    "\n",
    "The data in works in this assignment is related with the marketing campaigns of a Portuguese banking institution. The data has various features that gives information regarding the people that were contacted. The target variable in this dataset is the 'yes/no' column that depicts whether the person contacted subscribed to the bank term deposit or not. A machine learning model using Logistic Regression and naive Bayes classification methods will be implemented to predict the responses of the people contacted based upon the feature information provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle   \n",
    "import sys\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the pickle file that contains the data\n",
    "with open('bankTrain.pickle','rb') as inFile:\n",
    "    bankData=pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(40570, 22)\n",
      "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
      "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
      "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
      "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y', 'bankID'],\n",
      "      dtype='object')\n",
      "age                 int64\n",
      "job                object\n",
      "marital            object\n",
      "education          object\n",
      "default            object\n",
      "housing            object\n",
      "loan               object\n",
      "contact            object\n",
      "month              object\n",
      "day_of_week        object\n",
      "duration            int64\n",
      "campaign            int64\n",
      "pdays               int64\n",
      "previous            int64\n",
      "poutcome           object\n",
      "emp.var.rate      float64\n",
      "cons.price.idx    float64\n",
      "cons.conf.idx     float64\n",
      "euribor3m         float64\n",
      "nr.employed       float64\n",
      "y                  object\n",
      "bankID              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Should be true if the bankData is a pd DataFrame.\n",
    "print(isinstance(bankData,pd.DataFrame))   \n",
    "#information on the shape of the dataset\n",
    "print(bankData.shape)\n",
    "#Columns in the dataset that provide feature information that will be useful in modeling\n",
    "print(bankData.columns)\n",
    "#data types of the columns\n",
    "print(bankData.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age      duration      campaign         pdays      previous  \\\n",
      "count  40570.000000  40570.000000  40570.000000  40570.000000  40570.000000   \n",
      "mean      40.021863    258.428469      2.567833    962.457308      0.173305   \n",
      "std       10.421979    259.538128      2.772428    186.956234      0.495599   \n",
      "min       17.000000      0.000000      1.000000      0.000000      0.000000   \n",
      "25%       32.000000    102.000000      1.000000    999.000000      0.000000   \n",
      "50%       38.000000    180.000000      2.000000    999.000000      0.000000   \n",
      "75%       47.000000    320.000000      3.000000    999.000000      0.000000   \n",
      "max       98.000000   4918.000000     56.000000    999.000000      7.000000   \n",
      "\n",
      "       emp.var.rate  cons.price.idx  cons.conf.idx     euribor3m  \\\n",
      "count  40570.000000    40570.000000    40570.00000  40570.000000   \n",
      "mean       0.080658       93.575402      -40.50564      3.619832   \n",
      "std        1.571576        0.579181        4.63139      1.735075   \n",
      "min       -3.400000       92.201000      -50.80000      0.634000   \n",
      "25%       -1.800000       93.075000      -42.70000      1.344000   \n",
      "50%        1.100000       93.749000      -41.80000      4.857000   \n",
      "75%        1.400000       93.994000      -36.40000      4.961000   \n",
      "max        1.400000       94.767000      -26.90000      5.045000   \n",
      "\n",
      "        nr.employed        bankID  \n",
      "count  40570.000000  40570.000000  \n",
      "mean    5166.986495  20587.676091  \n",
      "std       72.269363  11899.158465  \n",
      "min     4963.600000      0.000000  \n",
      "25%     5099.100000  10268.250000  \n",
      "50%     5191.000000  20585.500000  \n",
      "75%     5228.100000  30894.750000  \n",
      "max     5228.100000  41187.000000  \n"
     ]
    }
   ],
   "source": [
    "print(bankData.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The describe function gives us quite a bit of information on the spread of the numeric values of some features. Right of the back we can see that the emp.var.rate, cons.price.idx, and nr.employed columns don't vary a lot and more likely are not good indicators for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    month\n",
       "no   apr       2062\n",
       "     aug       5427\n",
       "     dec         90\n",
       "     jul       6428\n",
       "     jun       4684\n",
       "     mar        268\n",
       "     may      12699\n",
       "     nov       3624\n",
       "     oct        399\n",
       "     sep        309\n",
       "yes  apr        531\n",
       "     aug        648\n",
       "     dec         88\n",
       "     jul        647\n",
       "     jun        551\n",
       "     mar        272\n",
       "     may        868\n",
       "     nov        413\n",
       "     oct        311\n",
       "     sep        251\n",
       "Name: bankID, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GroupBy = bankData.groupby(['y','month']).count()#[['day_of_week']]\n",
    "\n",
    "GroupBy['bankID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This shows a pretty even distribution for y grouped on day_of_week column. Thus showing that this feature has no real effect on the target value\n",
    "### Checks for missing or invalid data values, should go here. Look into suspicious. or unexpected, data types.\n",
    "\n",
    "There are no missing values in the entire dataset and hence we do not have to worry about imputing or deleting such null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "bankID            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are many unkowns in a few columns, we will not be imputing them as unknowns, too, can act as predictors. Since there are multiple features at play here, imputing these unkowns or removing those entries might harm the eventual rare event (target value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "We already decided to drop 'emp.var.rate', 'cons.price.idx', and 'nr.employed' from all the columns with numerical values due to their unvarying scale. However there are more features that logically will not contribute to the prediction of the target value.\n",
    "\n",
    "- 'default' feature has almost 80% values as 'no' and hence does not contribute to the prediction.\n",
    "- 'day_of_week' will be eliminiated as well due to the distribution shown above.\n",
    "- 'duration' column will be eliminated as well since the duration of the call cannot be known before the call and the y is known after the call; thus becoming a useless feature for designing a predictive model.\n",
    "- 'education' column has more than negligible amount of unknowns that return a 'yes' target value. However the 'job' column has lesser unkowns and in a way we assume relevance from job column should supercede information from education column. Thus we will not take the education column into consideration.\n",
    "\n",
    "Rearrange the columns and put the data in a new dataframe without eliminated features. Also transform the 'pdays' column into 3 categories to fetch a better understanding of the data. Also transforming the target variable \"yes\" to 1 and \"no\" to 0.\n",
    "\n",
    "Later each non-numeric categorical column with n distinct values will be converted into dummy variable n-1 columns. N-1 because the dummy variable trap will be avoided.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "bankData2 = bankData[['y','bankID','previous','age','euribor3m','job','marital','housing','loan',\n",
    "                      'contact','month','pdays','poutcome']]\n",
    "\n",
    "f1 = list(range(0,11))\n",
    "f2 = list(range(11,999))\n",
    "\n",
    "bankData2[\"pdays\"].replace(f1,\"ten.days.or.less\", inplace = True)\n",
    "bankData2[\"pdays\"].replace(f2,\"ten.days.or.more\", inplace = True)\n",
    "bankData2[\"pdays\"].replace([999],\"Never.Contacted\", inplace = True)\n",
    "bankData2[\"y\"].replace({\"yes\": 1, \"no\":0}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankData3=pd.get_dummies(bankData2, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As mentioned the following model will be trained using all the features except the ones that were analyzed and logically uninformative towards predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bankData3.iloc[:, 2:].values\n",
    "y = bankData3.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36513, 35)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(intercept_scaling=50, random_state=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_lg = LogisticRegression(intercept_scaling = 50,random_state = 0)\n",
    "classifier_lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier_lg.predict(X_test)\n",
    "ar =(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "print(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix as follow: \n",
      "[[3540   45]\n",
      " [ 387   85]]\n",
      "Accuracy Score 0.8935173773724427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix as follow: ')\n",
    "print(cm)\n",
    "print('Accuracy Score', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gaussian NB Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_nb = GaussianNB()\n",
    "classifier_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb = classifier_nb.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred_nb),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix as follow: \n",
      "[[3344  241]\n",
      " [ 269  203]]\n",
      "Accuracy Score 0.8742913482869115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "print('Confusion Matrix as follow: ')\n",
    "print(cm)\n",
    "print('Accuracy Score', accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We noticed that the Logistic Regression model yields a better accuracy than the Guassian Naive Bayes model. Next we will use the K folds method with both these algorithms to see if it yields a better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using k folds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import clone\n",
    "skf = StratifiedKFold(n_splits=40, random_state=99,shuffle=True) \n",
    "scaler=preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "logit_kf=LogisticRegression(solver='lbfgs')  \n",
    "guass_nb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "hold_logit = [] \n",
    "hold_guass = []\n",
    "\n",
    "for train_ndx, test_ndx in skf.split(X, y):\n",
    "    clone_clf = clone(logit_clf)\n",
    "    clone_guass = clone(guass_nb)\n",
    "    X_trainS=scaler.fit_transform(X[train_ndx])\n",
    "    y_train = y[train_ndx]\n",
    "    X_testS=scaler.transform(X[test_ndx])\n",
    "    y_test = y[test_ndx]\n",
    "\n",
    "    foldfitlogit=clone_clf.fit(X_trainS, y_train)\n",
    "    foldfitguass=clone_guass.fit(X_trainS, y_train)\n",
    "\n",
    "    y_pred_test_logit=foldfitlogit.predict(X_testS)\n",
    "    y_pred_train_logit=foldfitlogit.predict(X_trainS)\n",
    "    y_pred_test_guass=foldfitguass.predict(X_testS)\n",
    "    y_pred_train_guass=foldfitguass.predict(X_trainS)\n",
    "    \n",
    "    trainAcc_logit=accuracy_score(y_train,y_pred_train_logit)\n",
    "    testAcc_logit=accuracy_score(y_test,y_pred_test_logit)\n",
    "    trainAcc_guass=accuracy_score(y_train,y_pred_train_guass)\n",
    "    testAcc_guass=accuracy_score(y_test,y_pred_test_guass)\n",
    "\n",
    "    hold_logit.append({'train_accuracy':trainAcc_logit,'test_accuracy':testAcc_logit})\n",
    "    hold_guass.append({'train_accuracy':trainAcc_guass,'test_accuracy':testAcc_guass})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Folds method using Logistic Regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.899162</td>\n",
       "      <td>0.898496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.005226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.898878</td>\n",
       "      <td>0.883629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.899072</td>\n",
       "      <td>0.895464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.899178</td>\n",
       "      <td>0.898422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.899237</td>\n",
       "      <td>0.902367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.899459</td>\n",
       "      <td>0.908284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_accuracy  test_accuracy\n",
       "count       40.000000      40.000000\n",
       "mean         0.899162       0.898496\n",
       "std          0.000142       0.005226\n",
       "min          0.898878       0.883629\n",
       "25%          0.899072       0.895464\n",
       "50%          0.899178       0.898422\n",
       "75%          0.899237       0.902367\n",
       "max          0.899459       0.908284"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('K Folds method using Logistic Regression')\n",
    "pd.DataFrame(hold_logit)[['train_accuracy','test_accuracy']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Folds method using Naive bayes Classification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.874987</td>\n",
       "      <td>0.874760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.008025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.874381</td>\n",
       "      <td>0.857988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.869919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.875011</td>\n",
       "      <td>0.872781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.875190</td>\n",
       "      <td>0.879714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.875468</td>\n",
       "      <td>0.893491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_accuracy  test_accuracy\n",
       "count       40.000000      40.000000\n",
       "mean         0.874987       0.874760\n",
       "std          0.000257       0.008025\n",
       "min          0.874381       0.857988\n",
       "25%          0.874826       0.869919\n",
       "50%          0.875011       0.872781\n",
       "75%          0.875190       0.879714\n",
       "max          0.875468       0.893491"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('K Folds method using Naive bayes Classification')\n",
    "pd.DataFrame(hold_guass)[['train_accuracy','test_accuracy']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_score = foldfitlogit.decision_function(X_testS)\n",
    "\n",
    "average_precision = average_precision_score(y_pred_test_logit, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "yTestPredProbs = foldfitlogit.predict_proba(X_testS)\n",
    "\n",
    "testAUC=roc_auc_score(y_pred_test_logit, yTestPredProbs[:,1])\n",
    "print('Test AUC: {0:4.3f}'.format(testAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From all the analysis it's best to say that the 'foldfitlogit' Logistic Regression model trained using the K folds method provides the best accuracy of almost 90%. We will use that model on the test dataframe provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bankTest.pickle','rb') as inFile:\n",
    "    bankDataTest=pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same preprocessing of the data will need to happen on the test dataset before we predict results using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankData4 = bankDataTest[['y','bankID','previous','age','euribor3m','job','marital','housing','loan',\n",
    "                      'contact','month','pdays','poutcome']]\n",
    "\n",
    "f3 = list(range(0,11))\n",
    "f4 = list(range(11,999))\n",
    "\n",
    "bankData4[\"pdays\"].replace(f3,\"ten.days.or.less\", inplace = True)\n",
    "bankData4[\"pdays\"].replace(f4,\"ten.days.or.more\", inplace = True)\n",
    "bankData4[\"pdays\"].replace([999],\"Never.Contacted\", inplace = True)\n",
    "\n",
    "bankData5=pd.get_dummies(bankData4, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bankData5.iloc[:, 2:].values\n",
    "y = bankData5.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = foldfitlogit.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankID = bankData5.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bankID</th>\n",
       "      <th>pred_y</th>\n",
       "      <th>prob_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>40996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>40998.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>41112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>41118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.439020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>41132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>618 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bankID  pred_y    prob_y\n",
       "0       13.0     0.0  0.004217\n",
       "1      150.0     0.0  0.002259\n",
       "2      228.0     0.0  0.005192\n",
       "3      243.0     0.0  0.000365\n",
       "4      280.0     1.0  0.618428\n",
       "..       ...     ...       ...\n",
       "613  40996.0     0.0  0.005281\n",
       "614  40998.0     0.0  0.000718\n",
       "615  41112.0     0.0  0.004535\n",
       "616  41118.0     0.0  0.439020\n",
       "617  41132.0     0.0  0.000349\n",
       "\n",
       "[618 rows x 3 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = foldfitlogit.predict_proba(X)\n",
    "\n",
    "ar = np.concatenate((bankID.reshape(len(bankID),1), y_pred.reshape(len(y_pred),1),\n",
    "                     y_pred_prob[:,1].reshape(len(y_pred_prob),1)),1)\n",
    "\n",
    "FinalDf = pd.DataFrame(data=ar, columns = ['bankID','pred_y','prob_y'])\n",
    "FinalDf.rename(columns = {\"0\":\"bankID\",\"1\":\"pred_y\",\"2\":\"prob_y\"})\n",
    "FinalDf.astype({'bankID': 'int32','pred_y':'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDf.to_csv('preds_mehta_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Discussion, Conclusion and recommendations\n",
    "\n",
    "Receiving almost a 90% test accuracy is a great figure for most machine learning models. However this was a rare event dataset where the target value was a 'yes' only in about 11.3% (4580 'yes' out of 40570 total rows) of the cases. Thus we can safely say that if this model predicted 'no'as an asnwer for all test cases it would have been correct almost 88% of the times. Thus the accuracy of predicting of this model can be considered just average. In fact the average precision-recall and AUC score of the model come out as 1.00 which is extremely surprising. Out of the cut-down features that are currently considered in the model training, various features were taken out just to see how the accuracy of the model changes; but it didn't change considerably. One of the most prominent reasons could be improper feature scaling we utilized (MinMaxScaler). Another reason could be the abundance on the 'no' answers in the target variable column. Two recommendations I would like to give to the people that collect the data are 1) Do not have any unkowns in the dataset as those entries could have made a difference on how much the columns containing those unknown values might have had on the target prediction and 2) Retrieve more customer data regarding the possessions of the customer along with their debts as that will give a fair idea about their salaries. Salary can be one of the most deciding factors in deciding whether the customer would agree for a term deposit or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
