{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 422-57 Assignment 4- Prerak, Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction / Summary\n",
    "\n",
    "This assignment is focues on data collected to determine whether an early diagnosis of Parkinson's disease can be made using machine learning methods (Random Forest Regression and Lasso primarily in this assignment). It is a degenerative movement disorder disease for which no known cause is found. Based on serveral observations made at home on 42 people with early-stage Parkinson's disease during a 6 month trial period using a telemonitoring device, the dataset for this assignment is generated. The goal of the assignment is to predict at least one of the two target variables motor_UPDRS or total_UPDRS; a rating scale used to follow the longitudinal course of Parkinson's disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle   \n",
    "import sys\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the pickle file that contains the data\n",
    "with open('parkTrain.pickle','rb') as inFile:\n",
    "    parkData=pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(4993, 23)\n",
      "Index(['obsID', 'subjNo', 'age', 'sex', 'test_time', 'motor_UPDRS',\n",
      "       'total_UPDRS', 'Jitter(%)', 'Jitter(Abs)', 'Jitter:RAP', 'Jitter:PPQ5',\n",
      "       'Jitter:DDP', 'Shimmer', 'Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n",
      "       'Shimmer:APQ11', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'PPE'],\n",
      "      dtype='object')\n",
      "obsID              int64\n",
      "subjNo             int64\n",
      "age                int64\n",
      "sex                int64\n",
      "test_time        float64\n",
      "motor_UPDRS      float64\n",
      "total_UPDRS      float64\n",
      "Jitter(%)        float64\n",
      "Jitter(Abs)      float64\n",
      "Jitter:RAP       float64\n",
      "Jitter:PPQ5      float64\n",
      "Jitter:DDP       float64\n",
      "Shimmer          float64\n",
      "Shimmer(dB)      float64\n",
      "Shimmer:APQ3     float64\n",
      "Shimmer:APQ5     float64\n",
      "Shimmer:APQ11    float64\n",
      "Shimmer:DDA      float64\n",
      "NHR              float64\n",
      "HNR              float64\n",
      "RPDE             float64\n",
      "DFA              float64\n",
      "PPE              float64\n",
      "dtype: object\n",
      "obsID            0\n",
      "subjNo           0\n",
      "age              0\n",
      "sex              0\n",
      "test_time        0\n",
      "motor_UPDRS      0\n",
      "total_UPDRS      0\n",
      "Jitter(%)        0\n",
      "Jitter(Abs)      0\n",
      "Jitter:RAP       0\n",
      "Jitter:PPQ5      0\n",
      "Jitter:DDP       0\n",
      "Shimmer          0\n",
      "Shimmer(dB)      0\n",
      "Shimmer:APQ3     0\n",
      "Shimmer:APQ5     0\n",
      "Shimmer:APQ11    0\n",
      "Shimmer:DDA      0\n",
      "NHR              0\n",
      "HNR              0\n",
      "RPDE             0\n",
      "DFA              0\n",
      "PPE              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Should be true if the bankData is a pd DataFrame.\n",
    "print(isinstance(parkData,pd.DataFrame))   \n",
    "#information on the shape of the dataset\n",
    "print(parkData.shape)\n",
    "#Columns in the dataset that provide feature information that will be useful in modeling\n",
    "print(parkData.columns)\n",
    "#data types of the columns\n",
    "print(parkData.dtypes)\n",
    "\n",
    "print(parkData.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows us that 1) the data has no 'funny' datatypes and 2) there are no null values that we need to take care of by imputing or deleting in any of the columns of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             obsID       subjNo          age          sex    test_time  \\\n",
      "count  4993.000000  4993.000000  4993.000000  4993.000000  4993.000000   \n",
      "mean   2930.522532    21.447226    64.829161     0.322251    92.437859   \n",
      "std    1698.025741    12.384291     8.822902     0.467385    53.188345   \n",
      "min       0.000000     1.000000    36.000000     0.000000    -4.262500   \n",
      "25%    1464.000000    10.000000    58.000000     0.000000    46.847000   \n",
      "50%    2927.000000    21.000000    65.000000     0.000000    91.302000   \n",
      "75%    4396.000000    33.000000    72.000000     1.000000   137.830000   \n",
      "max    5874.000000    42.000000    85.000000     1.000000   215.490000   \n",
      "\n",
      "       motor_UPDRS  total_UPDRS    Jitter(%)  Jitter(Abs)   Jitter:RAP  ...  \\\n",
      "count  4993.000000  4993.000000  4993.000000  4993.000000  4993.000000  ...   \n",
      "mean     21.253019    28.941679     0.006164     0.000044     0.002992  ...   \n",
      "std       8.121940    10.665892     0.005729     0.000036     0.003187  ...   \n",
      "min       5.037700     7.000000     0.000830     0.000002     0.000330  ...   \n",
      "25%      14.890000    21.362000     0.003580     0.000022     0.001580  ...   \n",
      "50%      20.839000    27.489000     0.004900     0.000034     0.002250  ...   \n",
      "75%      27.594000    36.029000     0.006800     0.000054     0.003280  ...   \n",
      "max      39.511000    54.992000     0.099990     0.000446     0.057540  ...   \n",
      "\n",
      "       Shimmer(dB)  Shimmer:APQ3  Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA  \\\n",
      "count  4993.000000   4993.000000   4993.000000    4993.000000  4993.000000   \n",
      "mean      0.311203      0.017183      0.020165       0.027514     0.051550   \n",
      "std       0.231748      0.013370      0.016800       0.020112     0.040111   \n",
      "min       0.026000      0.001610      0.001940       0.002490     0.004840   \n",
      "25%       0.177000      0.009340      0.010850       0.015750     0.028010   \n",
      "50%       0.253000      0.013640      0.015860       0.022730     0.040910   \n",
      "75%       0.364000      0.020510      0.023620       0.032720     0.061520   \n",
      "max       2.107000      0.162670      0.167020       0.275460     0.488020   \n",
      "\n",
      "               NHR          HNR         RPDE          DFA          PPE  \n",
      "count  4993.000000  4993.000000  4993.000000  4993.000000  4993.000000  \n",
      "mean      0.032226    21.682367     0.541345     0.652921     0.219677  \n",
      "std       0.060788     4.314524     0.100371     0.071188     0.091156  \n",
      "min       0.000286     1.659000     0.151020     0.514680     0.021983  \n",
      "25%       0.010952    19.405000     0.471150     0.595450     0.156770  \n",
      "50%       0.018427    21.931000     0.542080     0.643420     0.205820  \n",
      "75%       0.031345    24.425000     0.613390     0.711400     0.264100  \n",
      "max       0.748260    37.875000     0.947920     0.865600     0.731730  \n",
      "\n",
      "[8 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(parkData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      obsID  subjNo  age  sex  test_time  motor_UPDRS  total_UPDRS  Jitter(%)  \\\n",
      "4386   4386      32   36    1     52.422       11.087       13.087    0.00358   \n",
      "5647   5647      41   68    1     35.480       32.012       40.012    0.00880   \n",
      "3537   3537      26   49    0    151.930       23.461       29.102    0.00546   \n",
      "2375   2375      17   66    1     55.309       27.594       33.594    0.00473   \n",
      "2930   2930      22   57    1     26.772       10.529       11.941    0.00258   \n",
      "\n",
      "      Jitter(Abs)  Jitter:RAP  ...  Shimmer(dB)  Shimmer:APQ3  Shimmer:APQ5  \\\n",
      "4386     0.000016     0.00192  ...        0.125       0.00515       0.00643   \n",
      "5647     0.000056     0.00492  ...        0.425       0.02055       0.02370   \n",
      "3537     0.000051     0.00273  ...        0.259       0.01570       0.01871   \n",
      "2375     0.000020     0.00266  ...        0.164       0.00862       0.00957   \n",
      "2930     0.000012     0.00115  ...        0.164       0.00683       0.00777   \n",
      "\n",
      "      Shimmer:APQ11  Shimmer:DDA       NHR     HNR     RPDE      DFA      PPE  \n",
      "4386        0.00922      0.01546  0.012798  22.223  0.48404  0.51655  0.12017  \n",
      "5647        0.04187      0.06166  0.034160  17.204  0.59668  0.69984  0.28993  \n",
      "3537        0.02340      0.04709  0.018401  21.491  0.60605  0.72467  0.22683  \n",
      "2375        0.01183      0.02585  0.008711  24.920  0.48785  0.59375  0.12938  \n",
      "2930        0.01141      0.02048  0.003961  27.790  0.28686  0.65748  0.11653  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(parkData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = parkData.iloc[:, [2,3,4,7,8,9,10,11,12,13,14,15,16,17,18,18,20,21,22]].values\n",
    "y_motor = parkData.iloc[:, 5].values\n",
    "y_total = parkData.iloc[:,6].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train_motor, y_test_motor = train_test_split(X, y_motor, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notes \n",
    "- The assignment allows us to choose 1 target variable from motor_UPDRS and total_UPDRS. We will work with motor_UPDRS as the target variable from here on. \n",
    "- We will evaluate the random forest regression model in two ways using k-fold cross validation (since that provides us the the best idea of how accurate our model is) i.e. 1) with max_features limit and 2) without max_features limit. We will use GridSearchCV to attain the hyperparameter max_depth for each of the two ways.\n",
    "- Other method we will use in the assignment is the Lasso method. We will use GridSearchCv to attain the value for hyperparameter alpha that will yield us the best accuracy. We will use K-fold to evaluate this model as well. \n",
    "- We will not be scaling either of our our models since Random Forest Regression and Lasso models don't require any scaling. \n",
    "- We will evaluate our RF regression model using R^2, MSE, OOB score (for RF regression), accuracy scoring (Lasso) and response variance on both training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 200, random_state = 0,bootstrap = True,oob_score = True\n",
    "                                  ,n_jobs = -1) \n",
    "\n",
    "regressormf = RandomForestRegressor(n_estimators = 200, random_state = 0,bootstrap = True,oob_score = True\n",
    "                                  ,n_jobs = -1, max_features = 'log2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, two different models were trained 1) with max features and 2) without max features. Notice that the max_depth parameter is missing here since we will determine it below (for both models) using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy of RF regression without max_features:  0.9660593844075901\n",
      "Best max_depth parameter for RF regression model without max_feature:  {'max_depth': 28}\n",
      "Best Accuracy of RF regression with max_features:  0.7308733755417113\n",
      "Best max_depth parameter for RF regression model with max_feature:  {'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'max_depth':[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}]\n",
    "search = GridSearchCV(estimator = regressor, param_grid = parameters, cv = 5)\n",
    "search.fit(X_train,y_train_motor)\n",
    "\n",
    "parametersmf = [{'max_depth':[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}]\n",
    "searchmf = GridSearchCV(estimator = regressormf, param_grid = parametersmf, cv = 5)\n",
    "searchmf.fit(X_train,y_train_motor)\n",
    "\n",
    "best_accuracy = search.best_score_\n",
    "best_parameters = search.best_params_\n",
    "print('Best Accuracy of RF regression without max_features: ', best_accuracy)\n",
    "print('Best max_depth parameter for RF regression model without max_feature: ', best_parameters)\n",
    "\n",
    "best_accuracymf = searchmf.best_score_\n",
    "best_parametersmf = searchmf.best_params_\n",
    "print('Best Accuracy of RF regression with max_features: ', best_accuracymf)\n",
    "print('Best max_depth parameter for RF regression model with max_feature: ', best_parametersmf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will re-train the models using the appropriate hyper parameter value attained from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 200, random_state = 0,bootstrap = True,oob_score = True\n",
    "                                  ,n_jobs = -1, max_depth = 28)\n",
    "\n",
    "regressormf = RandomForestRegressor(n_estimators = 200, random_state = 0,bootstrap = True,oob_score = True\n",
    "                                  ,n_jobs = -1, max_features = 'log2', max_depth = 18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result description of Random Forest Regression model without max_features limit:\n",
      "         trainMSE    testMSE    trainR2     testR2  OOB_Score  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    0.234762   1.695369   0.996441   0.974391   0.973695   \n",
      "std     0.010380   0.731705   0.000155   0.010186   0.001093   \n",
      "min     0.216796   0.874165   0.996016   0.943498   0.970804   \n",
      "25%     0.229850   1.236870   0.996369   0.969850   0.973274   \n",
      "50%     0.234141   1.498599   0.996436   0.977298   0.973458   \n",
      "75%     0.239678   1.968465   0.996521   0.981044   0.974116   \n",
      "max     0.262849   3.942558   0.996709   0.987338   0.975628   \n",
      "\n",
      "       Response_Variance_Train  Response_Variance_Test  \n",
      "count                20.000000               20.000000  \n",
      "mean                  0.996950                0.975400  \n",
      "std                   0.000224                0.010298  \n",
      "min                   0.996000                0.944000  \n",
      "25%                   0.997000                0.971000  \n",
      "50%                   0.997000                0.978000  \n",
      "75%                   0.997000                0.982000  \n",
      "max                   0.997000                0.988000  \n"
     ]
    }
   ],
   "source": [
    "#Using K-Fold Cross Validation to evaluate the Random Forest Regression Model without max_features limit\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=20,random_state=99,shuffle=True)\n",
    "X = parkData.iloc[:, [2,3,4,7,8,9,10,11,12,13,14,15,16,17,18,18,20,21,22]].to_numpy()\n",
    "y_motor = parkData.iloc[:, 5].to_numpy()\n",
    "\n",
    "cvres=[]  # Holder list for fold results\n",
    "\n",
    "for traindx, testdx in kf.split(X):  # loop over folds\n",
    "    resDict={}                       # Dictionary to hold fold results\n",
    "    XTrain = X[traindx]\n",
    "    yTrain_motor=y_motor[traindx]   \n",
    "    XTest = X[testdx]\n",
    "    yTest_motor=y_motor[testdx]\n",
    "    regModel=regressor.fit(XTrain,yTrain_motor) \n",
    "    trainPred=regModel.predict(XTrain)\n",
    "    trainR2=r2_score(yTrain_motor,trainPred)\n",
    "    trainMSE=mean_squared_error(yTrain_motor,trainPred)\n",
    "    testPred=regModel.predict(XTest)\n",
    "    testR2=r2_score(yTest_motor,testPred)\n",
    "    testMSE=mean_squared_error(yTest_motor,testPred)\n",
    "    ModelScore = regressor.oob_score_\n",
    "    df1 = pd.DataFrame(regressor.predict(XTest),columns = ['predict'])\n",
    "    df2 = pd.DataFrame(yTest_motor, columns =['test'])\n",
    "    RV_test = round(np.power(df2['test'].corr(df1['predict']),2),3)\n",
    "    df3 = pd.DataFrame(regressor.predict(XTrain),columns = ['predict'])\n",
    "    df4 = pd.DataFrame(yTrain_motor, columns =['train'])\n",
    "    RV_train = round(np.power(df4['train'].corr(df3['predict']),2),3)\n",
    "    resDict.update({'trainR2':trainR2,\n",
    "                    'testR2':testR2,\n",
    "                    'trainMSE':trainMSE,\n",
    "                    'testMSE':testMSE,\n",
    "                    'OOB_Score':ModelScore,\n",
    "                    'Response_Variance_Train':RV_train,\n",
    "                    'Response_Variance_Test':RV_test                    \n",
    "                   })\n",
    "    cvres.append(resDict)\n",
    "\n",
    "cvresDF=pd.DataFrame(cvres)[['trainMSE','testMSE','trainR2','testR2','OOB_Score','Response_Variance_Train'\n",
    "                             ,'Response_Variance_Test']]\n",
    "\n",
    "print('Result description of Random Forest Regression model without max_features limit:\\n' , cvresDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result description of Random Forest Regression model with max_features limit:\n",
      "         trainMSE    testMSE    trainR2     testR2  OOB_Score  \\\n",
      "count  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
      "mean    2.465113  15.518490   0.962622   0.763571   0.760468   \n",
      "std     0.039920   1.563782   0.000606   0.021696   0.003394   \n",
      "min     2.389185  12.720774   0.961620   0.730628   0.755131   \n",
      "25%     2.442349  14.710855   0.962152   0.750149   0.757713   \n",
      "50%     2.467291  15.340689   0.962624   0.759437   0.759593   \n",
      "75%     2.496482  16.703956   0.962940   0.778270   0.762659   \n",
      "max     2.528838  18.294845   0.963740   0.806908   0.767753   \n",
      "\n",
      "       Response_Variance_Train  Response_Variance_Test  \n",
      "count                20.000000               20.000000  \n",
      "mean                  0.980850                0.817750  \n",
      "std                   0.000489                0.019889  \n",
      "min                   0.980000                0.786000  \n",
      "25%                   0.981000                0.811000  \n",
      "50%                   0.981000                0.815000  \n",
      "75%                   0.981000                0.829500  \n",
      "max                   0.982000                0.853000  \n"
     ]
    }
   ],
   "source": [
    "#Using K-Fold Cross Validation to evaluate the Random Forest Regression Model with max_features limit\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=20,random_state=99,shuffle=True)\n",
    "X = parkData.iloc[:, [2,3,4,7,8,9,10,11,12,13,14,15,16,17,18,18,20,21,22]].to_numpy()\n",
    "y_motor = parkData.iloc[:, 5].to_numpy()\n",
    "\n",
    "cvres=[]  # Holder list for fold results\n",
    "\n",
    "for traindx, testdx in kf.split(X):  # loop over folds\n",
    "    resDict={}                       # Dictionary to hold fold results\n",
    "    XTrain = X[traindx]\n",
    "    yTrain_motor=y_motor[traindx]   \n",
    "    XTest = X[testdx]\n",
    "    yTest_motor=y_motor[testdx]\n",
    "    regModelmf=regressormf.fit(XTrain,yTrain_motor) \n",
    "    trainPred=regModelmf.predict(XTrain)\n",
    "    trainR2=r2_score(yTrain_motor,trainPred)\n",
    "    trainMSE=mean_squared_error(yTrain_motor,trainPred)\n",
    "    testPred=regModelmf.predict(XTest)\n",
    "    testR2=r2_score(yTest_motor,testPred)\n",
    "    testMSE=mean_squared_error(yTest_motor,testPred)\n",
    "    ModelScore = regressormf.oob_score_\n",
    "    df1 = pd.DataFrame(regressormf.predict(XTest),columns = ['predict'])\n",
    "    df2 = pd.DataFrame(yTest_motor, columns =['test'])\n",
    "    RV_test = round(np.power(df2['test'].corr(df1['predict']),2),3)\n",
    "    df3 = pd.DataFrame(regressormf.predict(XTrain),columns = ['predict'])\n",
    "    df4 = pd.DataFrame(yTrain_motor, columns =['train'])\n",
    "    RV_train = round(np.power(df4['train'].corr(df3['predict']),2),3)\n",
    "    resDict.update({'trainR2':trainR2,\n",
    "                    'testR2':testR2,\n",
    "                    'trainMSE':trainMSE,\n",
    "                    'testMSE':testMSE,\n",
    "                    'OOB_Score':ModelScore,\n",
    "                    'Response_Variance_Train':RV_train,\n",
    "                    'Response_Variance_Test':RV_test                    \n",
    "                   })\n",
    "    cvres.append(resDict)\n",
    "\n",
    "cvresDF=pd.DataFrame(cvres)[['trainMSE','testMSE','trainR2','testR2','OOB_Score','Response_Variance_Train'\n",
    "                             ,'Response_Variance_Test']]\n",
    "\n",
    "print('Result description of Random Forest Regression model with max_features limit:\\n' , cvresDF.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above analysis we can see that we're better off with all sorts of accuracy and error tests if we don't account for the max_features parameter in the random forest regression model. This is a little contradicting to the fact that having max_features limit (less than the number of features used to train the model) will not lead to any overfitting. However from the R^2 and variance test we can conclude that in this case more features means more information the model has to train on and provide a better result.   \n",
    "\n",
    "## Lasso Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "lasso = Lasso(max_iter=100000)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X_trainS = sc.fit_transform(X_train)\n",
    "X_testS = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy of lasso:  0.141044277438628\n",
      "Best alpha parameter for lasso model:  {'alpha': 0.004}\n"
     ]
    }
   ],
   "source": [
    "parametersla = [{'alpha':[0.0001,0.005,0.001,0.01,0.1,0.02,0.03,0.04,0.05,0.06,0.002,0.003,0.004,0.005,0.006,5]}]\n",
    "searchla = GridSearchCV(estimator = lasso, param_grid = parametersla, cv = 5)\n",
    "searchla.fit(X_trainS,y_train_motor)\n",
    "\n",
    "best_accuracy = searchla.best_score_\n",
    "best_parameters = searchla.best_params_\n",
    "print('Best Accuracy of lasso: ', best_accuracy)\n",
    "print('Best alpha parameter for lasso model: ', best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The accuracy seems way too low. Let's retrain the model and evaluate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.15\n",
      "Test set score: 0.15\n",
      "Number of features used: 12\n"
     ]
    }
   ],
   "source": [
    "lasso004 = Lasso(alpha=0.004, max_iter=100000).fit(X_trainS, y_train_motor)\n",
    "print(\"Training set score: {:.2f}\".format(lasso004.score(X_trainS, y_train_motor)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso004.score(X_testS, y_test_motor)))\n",
    "print(\"Number of features used:\", np.sum(lasso004.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence we can confirm the the Random Forest Regression model 'regModel' without the usage of max_features parameters is the best way to go to get the highest accuracy and least errors out of both the models we used. Next we will train our best model on the entire data set and see what result we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.06 11.09]\n",
      " [32.04 32.01]\n",
      " [23.33 23.46]\n",
      " ...\n",
      " [18.72 18.49]\n",
      " [34.99 35.  ]\n",
      " [17.72 17.75]]\n"
     ]
    }
   ],
   "source": [
    "y_motor_predict_final = regModel.predict(X)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_motor_predict_final.reshape(len(y_motor_predict_final),1), \n",
    "                      y_motor.reshape(len(y_motor),1)),1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainR2:  0.9958991819766205\n",
      "trainMSE:  0.27046001121804686\n",
      "\n",
      "Full Random Forest Prop of Entire Set Variance Accounted for:  0.996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "\n",
    "trainR2=r2_score(y_motor,y_motor_predict_final)\n",
    "trainMSE=mean_squared_error(y_motor,y_motor_predict_final)\n",
    "\n",
    "df1 = pd.DataFrame(regModel.predict(X),columns = ['predict'])\n",
    "\n",
    "df2 = pd.DataFrame(y_motor, columns =['test'])\n",
    "\n",
    "full_rf_test_result = round(np.power(df2['test'].corr(df1['predict']),2),3)\n",
    "\n",
    "print('trainR2: ', trainR2)\n",
    "print('trainMSE: ', trainMSE)\n",
    "print('\\nFull Random Forest Prop of Entire Set Variance Accounted for: ', full_rf_test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We received very positive results from applying our model to the entire data set. Now we will apply this model to our test pickle file to receive the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parkTest.pickle','rb') as inFile:\n",
    "    parkDataTest=pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testfinal = parkDataTest.iloc[:, [2,3,4,7,8,9,10,11,12,13,14,15,16,17,18,18,20,21,22]].to_numpy()\n",
    "\n",
    "y_pred_motor = regModelfinal.predict(X_testfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obsID            int32\n",
       "motor_UPDRS    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsID = parkDataTest.iloc[:, 0].values\n",
    "\n",
    "ar = np.concatenate((obsID.reshape(len(obsID),1), y_pred_motor.reshape(len(y_pred_motor),1)),1)\n",
    "\n",
    "FinalDf = pd.DataFrame(data=ar, columns = ['obsID','motor_UPDRS'])\n",
    "FinalDf.rename(columns = {\"0\":\"obsID\",\"1\":\"motor_UPDRS\"})\n",
    "FinalDf.astype({'obsID': 'int32','motor_UPDRS':'float64'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDf.to_csv('prerakmehta-assign-4-motor_UPDRS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Discussion, Conclusion and recommendations\n",
    "\n",
    "We received an extremely good accuracy via R^2 for our Random Forest Regression model. The main purposes of this assignment were to prepare a machine learning model to predict the target values in the test set as accurately as possible and avoid data leakage. We achieved these goals by using a Random Forest Regression model and other techniques such as GridSearchCV to find the most optimal hyper parameters in the RF model. We avoided the possibility of data leakage by using k-fold cross validation method and attaining mean of accuracies, MSE, variance, Out of Bag score and Response variance scores for both test and train sets. Afterwards we applied our best model on the entire data set and saw that we received very positive results. We can conclude that this isn't a case of overfitting because we saw the test accuracies and errors being very promising in the k-fold cross validation method. To my surprise the lasso method provided extremely poor results even after standardization. Regardless it would have been really challenging to beat the accuracy of the Random Forest Regression model. It provided an easiness in assigning relative importance to input features and also uses ensemble learning method for regression (and classification). Also it can handle numerous input variables without the need of variable deletion.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
